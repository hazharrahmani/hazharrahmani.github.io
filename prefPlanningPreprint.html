<html>
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FMPS3C294H"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FMPS3C294H');
</script>
<title>Preference-Based Planning in Stochastic Environments: From  Partially-Ordered  Temporal Goals to Most Preferred Policies</title>
<link rel="stylesheet" href="paperstyle.css" type="text/css">
</head>
<body>
<h2>Preference-Based Planning in Stochastic Environments: From  Partially-Ordered  Temporal Goals to Most Preferred Policies</h2>
<b>Hazhar Rahmani</b>, Abhishek N. Kulkarni, and Jie Fu<br>
Preprint <br>
2024
<h3>Abstract</h3>
Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTLf).  
    To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to transform a partially ordered preference over temporal goals into a computational model, called preference automaton, which is a semi-automaton with a partial order over acceptance conditions. In the second step, we prove that finding a most preferred policy is equivalent to computing a Pareto-optimal policy in a multi-objective MDP that is constructed from the original MDP, the preference automaton, and the chosen stochastic ordering relation. Throughout the paper, we employ running examples to illustrate the proposed preference specification and solution approaches. We demonstrate the efficacy of our algorithm using these examples, providing detailed analysis, and then discuss several potential future directions.	
<p><center>
<!-- <video style="width:600px; max-width:80%" src="RahOKa20b.mp4" controls></video> -->
  <!--
<iframe width="600px" height="400px" src="https://www.youtube.com/embed/0H0hvQpbFp8 " > </iframe>
  -->
</center></p>

<h3>Download</h3>
<a onClick="javascript: _gaq.push(['_trackPageview', 'preferenceBasedPlanningPreprint.pdf']);"  href="preferenceBasedPlanningPreprint.pdf"> 
<!--<a target="_blank" href="https://ieeexplore.ieee.org/document/8460507"> -->
<img src="pdf.png" border=0>
</a>
<h3>BibTeX</h3>
<pre>

@misc{rahmani2024preference,
  title={Preference-Based Planning in Stochastic Environments: From  Partially-Ordered  Temporal Goals to Most Preferred Policies},
  author={Rahmani, Hazhar and Kulkarni, Abhishek N and Fu, Jie},
  note={Preprint},
  year={2024}
}


</pre>
<div align=right>
<a href="index.html">Hazhar's home page</a><br>
</div>
<hr>
</font></i></div>
</body>
</html>
